\documentclass[10pt,landscape]{article}
\usepackage{multicol}
\usepackage{calc}
\usepackage{ifthen}
\usepackage[landscape]{geometry}
\usepackage{amsmath,amsthm,amsfonts,amssymb}
\usepackage{color,graphicx,overpic}
\usepackage{hyperref}
\usepackage{mathtools} % for 'bmatrix*' env.; loads 'amsmath' package automatically

\usepackage[dvipsnames]{xcolor}

\pdfinfo{
  /Title (example.pdf)
  /Creator (TeX)
  /Producer (pdfTeX 1.40.0)
  /Author (Huck)
  /Subject (Example)
  /Keywords (pdflatex,latex,pdftex,tex)}

% This sets page margins to .5 inch if using letter paper, and to 1cm
% if using A4 paper. (This probably isn't strictly necessary.)
% If using another size paper, use default 1cm margins.
\ifthenelse{\lengthtest { \paperwidth = 11in}}
    { \geometry{top=.5in,left=.5in,right=.5in,bottom=.5in} }
    {\ifthenelse{ \lengthtest{ \paperwidth = 297mm}}
        {\geometry{top=1cm,left=1cm,right=1cm,bottom=1cm} }
        {\geometry{top=1cm,left=1cm,right=1cm,bottom=1cm} }
    }

% Turn off header and footer
\pagestyle{empty}

\newcommand{\Laplace}[1]{\ensuremath{\mathcal{L}{\left[#1\right]}}}
\newcommand{\InvLap}[1]{\ensuremath{\mathcal{L}^{-1}{\left[#1\right]}}}

\newcommand{\R}{\mathbb{R}}
\newcommand{\C}{\mathbb{C}}


% Redefine section commands to use less space
\makeatletter
\renewcommand{\section}{\@startsection{section}{1}{0mm}%
                                {-1ex plus -.5ex minus -.2ex}%
                                {0.5ex plus .2ex}%x
                                {\normalfont\large\bfseries}}
\renewcommand{\subsection}{\@startsection{subsection}{2}{0mm}%
                                {-1explus -.5ex minus -.2ex}%
                                {0.5ex plus .2ex}%
                                {\normalfont\normalsize\bfseries}}
\renewcommand{\subsubsection}{\@startsection{subsubsection}{3}{0mm}%
                                {-1ex plus -.5ex minus -.2ex}%
                                {1ex plus .2ex}%
                                {\normalfont\small\bfseries}}
\makeatother


% Don't print section numbers
\setcounter{secnumdepth}{0}


\setlength{\parindent}{0pt}
\setlength{\parskip}{0pt plus 0.5ex}

%My Environments
\newtheorem{example}[section]{Example}
% -----------------------------------------------------------------------

\begin{document}
\raggedright
\footnotesize
\begin{multicols}{3}


% multicol parameters
% These lengths are set only within the two main columns
%\setlength{\columnseprule}{0.25pt}
\setlength{\premulticols}{1pt}
\setlength{\postmulticols}{1pt}
\setlength{\multicolsep}{1pt}
\setlength{\columnsep}{2pt}

\begin{center}
     \Large{\underline{AE 584 Exam 1 Fall 2017}} \\
\end{center}


\section{\color{blue}{Math}}
\subsection{\color{ForestGreen}{Binomial Expansions}}
\begin{align*}
(a^2-b^2)&=(a-b)(a+b)\\
(a^3-b^3)&=(a-b)(a^2+ab+b^2)\\
(a^4-b^4)&=(a-b)(a+b)(a^2+b^2)\\
\end{align*}
\subsection{\color{ForestGreen}{Exponentials}}
\begin{align*}
e^{a}e^{b}&=e^{a+b}\\
(e^{x})^n&=e^{2x}\\
\end{align*}
\subsection{\color{ForestGreen}{Matricies}}
\begin{align*}
(A+B)^T&=A^T+B^T\\
(AB)^T&=B^TA^T\\
AB^T&=BA^T\\
(A^T)^{-1}&=(A^{-1})^T\\
\end{align*}
\subsubsection{\color{Orange}{Square Matricies}}
\begin{align*}
det(A^T)&=det(A)\\
\end{align*}
Also, if $A$ is square, then its eigenvalues are equal to the eigenvalues of its transpose. Additionally, if the matrix is also differentiable and nonsingular
\begin{align*}
\frac{d}{dt}(P^{-1}(t)=-P^{-1}(t)\dot{P}(t)P^{-1}(t)
\end{align*}
which can be found with; $\frac{d}{dt}(P(t)P^{-1}(t))=\frac{d}{dt}(I)=0$
\subsubsection{\color{Orange}{Symetric Matricies}}
\begin{align*}
A^T&=A
\end{align*}
\subsubsection{\color{Orange}{Skew-Symetric Matricies}}
\begin{align*}
A^T=-A
\end{align*}
\subsection{\color{ForestGreen}{Inverting a $2x2$ matrix}}
\[
A = \begin{bmatrix}
       a_{11} & a_{12} \\ 
       a_{21} & a_{22}
    \end{bmatrix}
\]
\[
A^{-1}=
\frac{1}{\lvert A\rvert}
       \begin{bmatrix*}[r]
           a_{22} & -a_{12} \\ 
          -a_{21} &  a_{11}
       \end{bmatrix*} \,.
\]
\subsection{\color{ForestGreen}{Determinant of a $3x3$ matrix}}
$$\left|B\right|=det(B)=\left|\begin{array}{ccc}a&b&c\\d&e&f\\g&h&i\end{array}\right|$$
$$=a\left|\begin{array}{cc}e&f\\h&i\end{array}\right|-b\left|\begin{array}{cc}d&f\\g&i\end{array}\right|+c\left|\begin{array}{cc}d&e\\g&h\end{array}\right|$$
\subsection{\color{ForestGreen}{Trig Identities}}
\begin{align*}
sin(2a)&=2cos(a)sin(a)\\
cos(2a)&=1-2sin^2(a)=2cos^2(a)-1\\
\end{align*}
\subsection{\color{ForestGreen}{Leibniz Integral Rule}}
States that the derivative of the integral is (for constant limits of integration):
\begin{align*}
\frac{d}{dt}(\int_{a}^{b} f(x,t) \,dt)&=\int_{a}^{b} \frac{\partial}{\partial x}f(x,t)\,dt
\end{align*}
States that the derivative of the integral is (for limits of integration that are not constant)
\begin{align*}
&\frac{d}{dt}(\int_{f(a)}^{f(b)} f(x,t) \,dt)=\\
&=f(x,b(x))\frac{d}{dx}b(x)-f(x,a(x))\frac{d}{dx}a(x)+\int_{f(a)}^{f(b)} \frac{\partial}{\partial x}f(x,t)\,dt
\end{align*}
Note: $t$ is a variable in the integration limit, so the above formula must be used to derive $\dot{P}(t)$.
\subsection{\color{ForestGreen}{Integrals}}
\begin{align*}
\int x^n\,dx &= \frac{1}{n+1}x^{n+1}\\
\int \frac{1}{x}\,dx &= \ln |x|\\
\int u\dot{v}\,dx &= uv - \int v du\\
\int e^x\,dx &= e^x \\
\int a^x\,dx &= \frac{1}{\ln a} a^x\\
\int \ln x\,dx &= x \ln x - x\\
\int \sin x\,dx &= -\cos x\\
\int \cos x\,dx &= \sin x\\
\int \tan x\,dx &= \ln |\sec x|\\ 
\int \sec x\,dx &= \ln |\sec x + \tan x|\\
\int \sec^2 x\,dx &= \tan x\\
\int \sec(x) \tan(x)\,dx &= \sec x\\
\end{align*}
\subsubsection{\color{Orange}{Polar Coordinates $(\theta,r)$}}
\begin{align*}
dxdy=rdrd\theta
\end{align*}

\subsubsection{\color{Orange}{u Substitution}}
don't forget to change the limits of integration!

\subsection{\color{ForestGreen}{Derivatives}}
\begin{align*}
\frac{d}{dt}(tan^-1(x))&=\frac{1}{1+x^2}\\
\end{align*}

\subsection{\color{ForestGreen}{Laplace X-Forms}}
\begin{align*}
f(t)&=\Laplace{f(t)}=F(s)\\
1&=\dfrac{1}{s}\\
\delta(t)&=1\\
\delta(t-t_0)&=e^{-st_0}\\
f'(t)&=sF(s) - f(0) \\
f^{n}(t)&=s^nF(s) - s^{(n-1)} f(0) - \cdots - f^{(n-1)}(0)\\
t^n (n=0,1,2,\dots)&=\dfrac{n!}{s^{n+1}}\\
\sin kt&=\dfrac{k}{s^2+k^2}\\
\cos kt&=\dfrac{s}{s^2+k^2} \\
e^{at}&=\dfrac{1}{s-a}\\
t^ne^{at}&=\dfrac{n!}{(s-a)^{n+1}}\\
e^{at}\sin kt&=\dfrac{k}{(s-a)^2+k^2}\\
e^{at}\cos kt&=\dfrac{s-a}{(s-a)^2+k^2}\\
t\sin kt&=\dfrac{2ks}{(s^2+k^2)^2}\\
t\cos kt&=\dfrac{s^2-k^2}{(s^2+k^2)^2} \\
\end{align*}
First translation theorom:
\begin{align*}
\Laplace{e^{at}f(t)}&=F(s-a)\\
\end{align*}

\subsection{\color{ForestGreen}{Delta Dirac Function}}
\begin{align*}
\int_{-\infty}^{\infty} f(x) \delta(x-a) \,dx&=f(a)\\
\int_{-\infty+\epsilon}^{\infty+\epsilon} f(x) \delta(x-a) \,dx&=f(a), \epsilon>0\\
\delta(x-a)&=0\\
\end{align*}

\section{\color{blue}{Linearization}}
Given some nonlinear system of the form:
\begin{align*}
\dot{x}(t)&=f(x(t),u(t),t)\\
y(t)&=g(x(t),t)
\end{align*}
A linearization can be performed about nominal trajectories $x^0(t)$, $u^0(t)$, and $u^0(t)$ (shorthand is $0$) by defining the jacobian of w.r.t. the states evaluated at the nominal trajectories ($0$) as:
\begin{align*}
A(t)= \frac{\partial f}{\partial x}\Bigr|_0= 
\begin{bmatrix}
    \frac{\partial f_1}{\partial x_1} & \frac{\partial f_1}{\partial x_2} & \dots  & \frac{\partial f_1}{\partial x_n} \\
    \frac{\partial f_2}{\partial x_1} & \frac{\partial f_2}{\partial x_2} & \dots  & \frac{\partial f_2}{\partial x_n} \\
    \vdots & \vdots & \ddots & \vdots \\
    \frac{\partial f_n}{\partial x_1} & \frac{\partial f_n}{\partial x_2} & \dots  & \frac{\partial f_n}{\partial x_n} \\
\end{bmatrix}\Bigr|_0
\end{align*}
As well as the jacobian of w.r.t. the controls evaluated at the nominal trajectories ($0$) as:
\begin{align*}
B(t)= \frac{\partial f}{\partial u}\Bigr|_0= 
\begin{bmatrix}
    \frac{\partial f_1}{\partial u_1} & \frac{\partial f_1}{\partial u_2} & \dots  & \frac{\partial f_1}{\partial u_m} \\
    \frac{\partial f_2}{\partial u_1} & \frac{\partial f_2}{\partial u_2} & \dots  & \frac{\partial f_2}{\partial u_m} \\
    \vdots & \vdots & \ddots & \vdots \\
    \frac{\partial f_n}{\partial u_1} & \frac{\partial f_n}{\partial u_2} & \dots  & \frac{\partial f_n}{\partial u_m} \\
\end{bmatrix}\Bigr|_0
\end{align*}

Finally, the output equation may need to be linearized about $0$ as well
\begin{align*}
C(t)= \frac{\partial g}{\partial x}\Bigr|_0= 
\begin{bmatrix}
    \frac{\partial g_1}{\partial x_1} & \frac{\partial g_1}{\partial x_2} & \dots  & \frac{\partial g_1}{\partial x_n} \\
    \frac{\partial g_2}{\partial x_1} & \frac{\partial g_2}{\partial x_2} & \dots  & \frac{\partial g_2}{\partial x_n} \\
    \vdots & \vdots & \ddots & \vdots \\
    \frac{\partial f_n}{\partial x_1} & \frac{\partial g_n}{\partial x_2} & \dots  & \frac{\partial g_n}{\partial x_n} \\
\end{bmatrix}\Bigr|_0
\end{align*}

Then the perterbation variables are defined as:
\begin{align*}
\delta x(t)=x(t)-x^0(t)\\
\delta u(t)=u(t)-u^0(t)\\
\delta y(t)=y(t)-y^0(t)
\end{align*}

The final linearized system is:
\begin{align*}
\delta \dot{x}(t)&=A(t)\delta x(t)+B(t)\delta u(t)\\
\delta y(t)&=C(t)\delta x(t)
\end{align*}


\section{\color{blue}{State Transition Matrix, $\Phi(t,t_0)$}}
\subsection{\color{ForestGreen}{Without intut to the system}}
\begin{align*}
\dot{x}(t)&=A(t)x(t)\\
x(t)&=\Phi(t,t_0)x_0\\
\Phi(t,t_0)&=x(t)x(t_0)^-1\\
\end{align*}

\subsection{\color{ForestGreen}{With input to the system}}
The system is
\begin{align}
\dot{x}(t)&=A(t)x(t)+B(t)u(t) \nonumber\\
y(t)&=C(t)x(t)\nonumber\\
x(t_0)&=x_0 \label{eq:sys1}
\end{align}
where the solution is,
\begin{align}
x(t)&=\Phi(t,t_0)x_0+\int_{t_0}^t \Phi(t,\tau)B(\tau)u(\tau)\,d\tau \label{eq:sol1}\\
y(t)&=C(t)\Phi(t,t_0)x_0+\int_{t_0}^t \underbrace{C(t)\Phi(t,\tau)B(\tau)}_{(G(t,\tau))} u(\tau)\,d\tau \nonumber
\end{align}

The above two equations are called the \textbf{variation of constants formulas}. They contain two terms, the first term is the \textbf{free response} which is due to $x_0$ and the second term is the \textbf{forced resonse} due to the input $u(t)$. Additionally, the \textbf{impulse response is} defined as
\begin{align}
G(t,\tau)=C(t)\Phi(t,t_0)B(\tau), \tau \leq t  \label{eq:impulse_response}
\end{align}

\subsection{\color{ForestGreen}{Finding $\Phi$: LTI Systems A}}
The basic equation is:
\begin{align*}
\Phi(t,\tau)&=e^{A(t-\tau)}\\
\end{align*}
Which can be calculated using:
\begin{align*}
\InvLap{(sI-A)^{-1}}&=e^{At}\\
\end{align*}
After this, $\tau$ must be added in, which can be done   with by taking the inverse of $\Phi(\tau,0)$ to get $\Phi(0,\tau)$ and then multiplying by $\Phi(t,0)$ as:
\begin{align*}
\Phi(t,0)\Phi(0,\tau)&=\Phi(t,\tau)\\
\end{align*}
\subsection{\color{ForestGreen}{Finding $\Phi$: LTV Systems A(t)}}
The STM is the unique solution to 
\begin{align*}
\frac{\partial}{\partial t}(\Phi(t,t_0))=A(t)\Phi(t,t_0)
\end{align*}
with inital conditions $\Phi(t_0,t_0) =I$.

To solve:
\begin{itemize}
\item multiply the above matrices out
\item take the Laplace Transform of each element in the matrix
\item solve the algebreic equation for each $\Phi_{i,i}(s)$
\item take the inverse Laplace transfrom to find $\Phi_{i,i}(t)$
\end{itemize}

\subsection{\color{ForestGreen}{Properties of STM}}
\begin{align*}
\Phi(t_0,t_0) &=I\\
\Phi(t,t_0)^-1 &=\Phi(t_0,t)\\
\Phi(t,t_0)&=\Phi(t,t_1)\Phi(t_1,t_0)\\
\end{align*}
\section{\color{blue}{Deterministic Asymtotic Observers}}

\section{\color{blue}{Stability}}
The system Eqn. \ref{eq:sys1} is stable if, given $x(t_0)=x_0$, $x(t)$ (Eqn. \ref{eq:sol1}) is bounded is bounded $\forall t \geq t_0$. In this case, $$lim_{t\to\infty} x(t)$$  may not go to zero. Stability can also be determined by looking at each $(i,j)$ component of the STM as:
\begin{align*}
|\Phi_{ij}(t,t_0)| \leq k < \infty, \forall t_0 \leq t
\end{align*}
\subsection{\color{ForestGreen}{Asymtotic Stability}}
The system Eqn. \ref{eq:sys1} is asymtotically stable if, given $x(t_0)=x_0$ $x(t)$, $x(t)$ (Eqn. \ref{eq:sol1}) decays to zero, that is:
$$lim_{t\to\infty} x(t) = 0$$ 
\subsection{\color{ForestGreen}{Unstable Systems}}

\section{\color{blue}{BIBO Stabile Systems}}
The system, Eqn. \ref{eq:sys1}, is BIBO stable if when $x_0=0$, the forced output response $y(y)$ to every bounded input $u(t)$ is bounded. This can be determined as:
\begin{align*}
\int_{-\infty}^{t} |G_{ij}(t,\tau)| d\tau \leq k < infty
\end{align*}
where, $G(t,\tau)$ was defined in Eqn. \ref{eq:impulse_response} and the above equation requires that $G(t,\tau)$ is "absolutely integrable."

\section{\color{blue}{Observabilty}}
Can we estimate a unique $x(t_0)=x_0$, given $u(t)$ and $y(t)$ over the time interval $[t_0,t_1]$? If we have $x_0$ we can solve 
\begin{align*}
x(t)&=\Phi(t,t_0)x_0+\int_{t_0}^t \Phi(t,\tau)B(\tau)u(\tau)\,d\tau\\
\end{align*}
The state is unobservable for the unforced system ($u(t)=0$), if:
\begin{align*}
y(t)=C(t)\Phi(t,t_0)x_0=0
\end{align*}
\subsection{\color{ForestGreen}{LTI Systems A}}
The observability matrix for a LTI system is:
\begin{align*}
\mathcal{O}=
\begin{bmatrix}
C\\
CA\\
\vdots \\
CA^{n-1}
\end{bmatrix}
\end{align*}
if the $rank(\mathcal{O})=n$ then the system is observable. The unobservable states are in the null space of the observability matrix, i.e. $\mathcal{O}x_0=0$
\subsection{\color{ForestGreen}{LTV Systems A(t)}}
Observability Gramian
\begin{align*}
M(t_0,t_1)=\int_{t_0}^{t_1} \Phi^T(t,t_0)C^T(t)C(t)\Phi^T(t_0,t)dt
\end{align*}
a state $x_0=x(t_0)$ unobservable at time $t_0$ iff
\begin{align*}
M(t_0,t_1)x_0=0,\forall t_1>t_0
\end{align*}
so, the unobservable states are in the null-space of the Observability Gramian. If the only solution that lives in the null-space is the zero vector $x(t_0)=0$, then the system is completely observable. Also, note that ther is no need to carry out the complete integral to see that the Observability Gramian will have unobservable states in its null space, i.e. integration does not change the form of the matrix. 
\section{\color{blue}{Controlability}}
A system is controllable if we can find a $u(t)$ that drives the state $x(t)$ from $x_0$ in finite time $t_f$.
\subsection{\color{ForestGreen}{LTI Systems A}}
The observability matrix for a LTI system is:
\begin{align*}
\mathcal{C}=
\begin{bmatrix}
B \; BA\; \dots\; BA^{n-1}
\end{bmatrix}
\end{align*}
if the $rank(\mathcal{C})=n$ then the system is controllable. Recall that the rank of a matrix is the number of linearly independent columns.
\subsection{\color{ForestGreen}{LTV Systems A(t)}}
Controllability Gramian
\begin{align*}
W(t_0,t_1)=\int_{t_0}^{t_1} \Phi^T(t,t_0)B(t)B^T(t)\Phi^T(t_0,t)dt
\end{align*}
this matrix is always symetric and positive definite.
The system is completly controllable if there exists $t_1>t_0$: $W(t_0,t_1)>0$. If the system is controllable at $t_0$, then one control that drives the state to the origin is:
\begin{align*}
u_0(t)=-B^T(t)\Phi^T(t_0,t)W^{-1}(t_0,t_1)x_0
\end{align*} 
\section{\color{blue}{Duality}}
Given a LTV system as
\begin{align*}
\dot{x}(t)&=A(t)x(t)+B(t)u(t)\\
y(t)&=C(t)x(t)
\end{align*}
its dual system is
\begin{align*}
\dot{x}(t)&=-A^T(t)x(t)+C^T(t)u(t)\\
y(t)&=B^T(t)x(t)
\end{align*}
The controllable (or uncontrollable) states of one system are the observable (or unobservable) states of the other system.
\section{\color{blue}{Random Vectors}}
If $x$ is a random vector with a PDF $f(x)$ and $g:\R_n \rightarrow \R_m$ is a function of $x$ 
\subsection{\color{ForestGreen}{Expected Value}}
\begin{align*}
E[g(x)]&=\int_{\R_n} g(x)f(x)\,dx \in \R_m\\
\end{align*}
\subsection{\color{ForestGreen}{Mean Value}}
\begin{align*}
\bar{x}&=E[x]=\int_{\R_n} xf(x)\,dx \in \R_n
\end{align*}
\subsection{\color{ForestGreen}{Covariance}}
\begin{align*}
P_{xx}&=E[(x-\bar{x})(x-\bar{x})^T]\\
&=\int_{\R_n} (x-\bar{x})(x-\bar{x})^T f(x)\,dx \in \R_{nxn}\\
\end{align*}
Note: the covariance matrix is symmetric as well as positive semidefinite, so
\begin{align*}
\forall v \in \R_n, v^TP_xv\geq0
\end{align*}
Thus,
\begin{align*}
v^TP_xv&=\int_{R_n} v^T (x-\bar{x})(x-\bar{x})^Tvf(x)\,dx\\
&=\int_{R_n} (x-\bar{x})^2f(x)\,dx \geq 0
\end{align*}
\subsection{\color{ForestGreen}{Cross-Covariance}}
If $x1$ and $x2$ are subvectors of $x$
\begin{align*}
P_{x_1x_2}&=E[(x_1-\bar{x_1})(x_2-\bar{x_2})^T]\\
&=\int_{\R_n} (x_1-\bar{x_1})(x_2-\bar{x_2})^T f(x)\,dx \in \R_{n_1xn_2}\\
\end{align*}

\subsection{\color{ForestGreen}{Variance of Error}}
\begin{align*}
P^+=((P^-)^{-1}+C^TR^{-1}C)^{-1}
\end{align*}

\subsection{\color{ForestGreen}{Probability Density Functions}}
The relative likelyhood that a random variable $x$ will take on vaules on a given interval.
\begin{align*}
Area=P(a \leq x \leq b) = \int_a^b f_x(x) dx
\end{align*}
\subsubsection{\color{Orange}{Properties of PDF}}
If you integrate over the PDF over the entire range then it must equal $1$ and the PDF must always be greater than $0$.
\begin{align*}
\int_{\R} f_x dx=1
\forall x, f_x(x) \geq 0
\end{align*}

\subsubsection{\color{Orange}{Uniform PDF}}
For a PDF uniformly distributed over $[a,b]$, $f(x)=constant=c$. $c$ can then be determined by $\int_a^b c dx=1$ which results in $f(x)=\frac{1}{b-a}$. 
\subsubsection{\color{Orange}{Marginal PDF}}
Given the joint PDF $f_x(x)=f_x(x_1,x_2)$, the marginal PDF of $x_1$ is:
\begin{align*}
f_{x_1}(x_1)=\int_{-\infty}^{\infty} f_x(x_1,x_2) dx_2
\end{align*}
\subsection{\color{ForestGreen}{Characteristic Function, $\phi_x(s)$}}
$\phi_x(s)$ is useful to compute the PDF for $x$ and the Gaussian distribution. The expected value can be used to calculate the characteristic function as: 
\begin{align*}
\phi_x(s)&=E[e^{jx^Ts}]=\int_{\R_{n}} e^{jx^Ts}f(x)\,dx\\
\end{align*}
where $j^2=-1$ and $s$ is a complex vector of order $n$

The statistal properties of $x$ are equivalently specified by PDF $f(x)$ or by the characteristic function $\phi_x(s)$. 

\subsubsection{\color{Orange}{Inversion of, $\phi_x(s)$}}
Similar to a Fourier Transform $\phi_x(s)$ can be put back into the time domain with: 
\begin{align*}
f(x)&=\frac{1}{(2 \pi)^n}\int_{\R_{n}} e^{jx^Ts} \phi_x(s),ds\\
\end{align*}


\subsection{\color{ForestGreen}{Independence}}

If the following equations are true, then the PDF's are independent.
\begin{align*}
f(x,y)&=f_x(x)f_y(y)\\
\phi_{xy}(s,r)&=\phi_x(s)\phi_y(r)\\
\end{align*}

If $x$ and $y$ are independent, the conditional density function and conditional mean satisfy:
\begin{align*}
f(x|y)&=\frac{f(x,y)}{f_y(y)}=\frac{f_x(x)f_y(y)}{f_y(y)}=f_x(x)\\
E[x|y]&=\int_{\R_{n}} xf_x(x)\,dx=E[x]=\bar{x}
\end{align*}

\subsection{\color{ForestGreen}{Correlation}}
If x and y are independent, then they are uncorrelated. But, if they are uncorrelated, they may not be independent!\\
If $x$ and $y$ are uncorrelated, then they satisfy:
\begin{align*}
E[xy^T]=E[x]E[y^T]
\end{align*}

If $x$ and $y$ are uncorrelated, the cross-covariances must be zero:
\begin{align*}
P_{xy}&=E[(x-\bar{x})(y-\bar{y})^t]\\
&=E[xy^T-x\bar{y}^T-\bar{x}y+\bar{x}\bar{y}^T]\\
&=E[xy^T]-E[x]\bar{y}^T-\bar{x}E[y^T]+\bar{x}\bar{y}^T\\
&=E[xy^T]-\bar{x}\bar{y}^T-\bar{x}\bar{y}^T+\bar{x}\bar{y}^T\\
&=E[xy^T]-\bar{x}\bar{y}^T=0\\
\end{align*}

\section{\color{blue}{Gaussian Distribution}}
Why model the probablity densit function as a Gaussian (or normal) distribution:
\begin{itemize}
\item provides a good statistical model for many natural phenomina
\item computationally tractable because the statistical properties are described completely by first (mean, $\bar{x}$) and second (variance, $P$) moments 
\item normality is preserved through linear transforms (both static and dynamic)
\end{itemize} 

\subsection{\color{ForestGreen}{Characteristic Function, \textbf{for a Gaussian vector}}}
A random vector $x \in \R_n$ is Gaussian distributed or normal if the characteristic function has the form:
\begin{align*}
\phi_x(s)=e^{j\bar{x}^Ts-\frac{1}{2}s^TPs}
\end{align*}
where $s \in \, C_n$, $\bar{x}=E[x]$, and $P=[(x-\bar{x})(x-\bar{x})^T]$.
\begin{itemize}
\item For such a random vector, we use the notation $x=N(\bar{x},P)$. 
\item Two vectors $x$ and $y$ are jointly Gaussian distributed if $(x^T,y^T)$ is Gaussian.
\item When a random vector $x$ is Gaussian and it's covariance matrix $P_x$ is nonsingular, it's PDF can be evaluated with:
\begin{align*}
f(x)=\frac{e^{-\frac{1}{2}(x-\bar{x})^TP_x^{-1}(x-\bar{x})}}{\sqrt{(2\pi)^ndet(P_x)}}
\end{align*}
\end{itemize}

If $P_x$ is singular, then the above equation will not work, but the characteristic function can define the Gaussian distribution indirectly. 
\section{\color{blue}{Random Process}}
In a random process, we are looking at a family of random vectors ($x(t),t\in I$) indexed by time. 

\subsection{\color{ForestGreen}{Mean Value Function}}
\begin{align*}
\bar{x}(t)&=E[x(t)], t \in I
\end{align*}

\subsection{\color{ForestGreen}{Covariance Kernal}}
\begin{align*}
P(t,\tau)=E[(x(t)-\bar{x}(t))(x(t)-\bar{x}(t))^T],t \in I
\end{align*}

\subsection{\color{ForestGreen}{Cross-Covariance Kernal}}
For two random processes $x(t)$ and $y(t)$
\begin{align*}
P_{xy}(t,\tau)=E[(x(t)-\bar{x}(t))(y(t)-\bar{y}(t))^T],t \in I
\end{align*}

\subsection{\color{ForestGreen}{Covariance Matrix $= P(t,t)$}}
Which satisfies:
\begin{align*}
P(t)=E[x(t)x^T(t)]-\bar{x}(t)\bar{x}^T(t)
\end{align*}
\subsection{\color{ForestGreen}{Cross-Covariance Matrix $= P_{xy}(t,t)$}}
Which satisfies:
\begin{align*}
P_{xy}(t)=E[x(t)y^T(t)]-\bar{x}(t)\bar{y}^T(t)
\end{align*}

\subsection{\color{ForestGreen}{Independence}}
todo..
\subsection{\color{ForestGreen}{Correlation}}
todo..
\section{\color{blue}{Gauss-Markov Process}}
This section combines the ideas of \textbf{Gaussian distribution} and \textbf{random process}
\begin{itemize}
\item a random process is $x(t)$ is \textbf{Gaussian} if all of the vectors $x_1(t)$,...$x_n(t)$ are jointly Gaussian
\item a Gaussian random process is \textbf{white} if the vectors $x(t_1)$, ...$x(t_m)$ are independent, otherwise it is \textbf{colored}
\item for a \textbf{Gaussian} and \textbf{white} process, the \textbf{covariance kernal} satisfies 
\begin{align*}
P(t,\tau)&=0,t \neq \tau\\
P(t,\tau)&=Q(t)\delta(t-\tau)
\end{align*}
\item a random process is \textbf{Markov} if 
\begin{align*}
&f(x(t_m)|x(t_{m-1}),...,x(t_1))=f(x(t_m)|x(t_{m-1}))
\end{align*}
\item a random process is \textbf{Gauss-Markov} if it is both \textbf{Gauss} and \textbf{Markov}
\end{itemize}

\section{\color{blue}{Linear Gauss-Markov Models}}
The standard model is:
\begin{align}
\dot{x}(t)&=A(t)x(t)+B(t)u(t)+w(t), t \geq t_0\\
y(t)&=C(t)x(t)+v(t)\label{eq:sys2}
\end{align}


\subsection{\color{ForestGreen}{Standard Assumptions}}
\begin{enumerate}
\item the intial condition $x(t_0)$ is Gaussian
\begin{align*}
x(t_0)&=N(\bar{x}(t_0),P(t_0))
\end{align*}
\item the disturbance $w(t)$ is a zero-mean, Gaussian, white process that is independent of $x(t_0)$
\begin{align*}
E[w(t)]&=0\\
E[w(t)w^T(\tau)]&=\underbrace{R_w(t)\delta(t-\tau)}_{covariance\;kernal}\\
E[w(t)(x(t_0-\bar{x}(t_0))^T]&=0
\end{align*}
\textbf{EX:} in the case of a $2X2$ system, if there is some covariance $\sigma_w$ given for the second state variable then 
\begin{align*}
R_w(t)=
\begin{bmatrix}
0 &0\\
0 &\sigma_w
\end{bmatrix}
\end{align*}

\item the measurment noinse $v(t)$ is a zero-mean, Gaussian, white process that is independent of $x(t_0)$
\begin{align*}
E[v(t)]&=0\\
E[v(t)v^T(\tau)]&=R_v(t)\delta(t-\tau)\\
E[v(t)(x(t_0-\bar{x}(t_0))^T]&=0
\end{align*}
\item the processes $v(t)$ and $w(t)$ are uncorrelated
\begin{align*}
E[w(t)v^T(t)]=0
\end{align*}
\end{enumerate}
With the standard model and assumptions, the process $x(t)$ is \textbf{Markov}.
\subsection{\color{ForestGreen}{Proof that $x(t)$ is Markov}}
Recall the variation of consants formula:
\begin{align*}
x(t)&=\Phi(t,t_0)x(t_0)+\int_{t_0}^{t} \Phi(t,\tau) B(\tau)u(\tau)d\tau +...\\ 
&+ \int_{t_0}^{t}\Phi(t,\tau)w(\tau)d\tau 
\end{align*}
Then,
\begin{align*}
x(t_m)&=\Phi(t_m,t_{m-1})x(t_{m-1})+...\\
&+ \int_{t_{m-1}}^{t_m} \Phi(t_m,\tau)(B(\tau)u(\tau)+ w(\tau))d\tau 
\end{align*}
Notice that the results does not depend on $x(\tau)$, $\tau < t_{m-1}$.

\subsection{\color{ForestGreen}{Mean Value Functions, for Eqn. \ref{eq:sys2}}}
Also refered to as the expected value.

\subsubsection{\color{Orange}{Mean Value Function, $\bar{x}(t)$}}
\begin{align*}
\bar{x}(t)=\Phi(t,t_0)\bar{x}(t_0)+\int_{t_0}^{t} \Phi(t,\tau) B(\tau)u(\tau)d\tau 
\end{align*}

\subsubsection{\color{Orange}{Mean Value Function, $\bar{y}(t)$}}

\begin{align*}
\bar{y}(t)=E[C(t)x(t)+v(t)]=C(t)\bar{x}(t)
\end{align*}

\subsection{\color{ForestGreen}{Covariance Matrixes, for Eqn. \ref{eq:sys2}}}
\subsubsection{\color{Orange}{Covariance Matrix, $P_x(t)=P(t)$}}

Using $x(t)-\bar{x}(t)$ we can define:
\begin{align*}
P(t)=\Phi(t,t_0)P(t_0)\Phi^T(t,t_0)+\int_{t_0}^{t} \Phi(t,\tau) R_w(\tau)\Phi^T(t,\tau)d\tau 
\end{align*}
The above equation also satisfies the Lyapunov Equation.

\subsubsection{\color{Orange}{Covariance Matrix, $P_y(t)$}}
For the output $y(t)$:

\begin{align*}
P_y(t)=C(t)P(t)C^T(t)+R_v(t)
\end{align*}

\subsection{\color{ForestGreen}{Lyapunov Equation}}
with the standard model and assumptions, the covariance matrix satisfies:
\begin{align*}
\dot{P}(t)&=A(t)P(t)+P(t)A^T(t)+R_w(t)
\end{align*}
Which can be derived using the \textbf{Leibniz Integral Rule}. 
\subsubsection{\color{Orange}{Steady State Covariance Matrix}}
To find the steady state covariance matrix:
\begin{itemize}
\item set $\dot{P}(t)=0$
\item then solve for $P_{ss}$
	\begin{itemize}
	\item this will require a computer!
	\end{itemize}
\end{itemize}

\subsection{\color{ForestGreen}{Covariance Kernals, for Eqn. \ref{eq:sys2}}}

\subsubsection{\color{Orange}{Covariance Kernal, $P_x(t,\tau)$}}
For the state $x(t)$:
\begin{align*}
P_x(t,\tau)=\Phi(t,t_0)P(t_0)\Phi^T(t,t_0)+\int_{t_0}^{t} \Phi(t,\sigma) R_w(\sigma)\Phi^T(t,\sigma)d\sigma 
\end{align*}

\subsubsection{\color{Orange}{Covariance Kernal, $P_y(t,\tau)$}}
For the output $y(t)$:
\begin{align*}
P_y(t,\tau)=C(t)\Phi(t,t_0)P(t_0)\Phi^T(t,t_0)C^T(\tau)+ \\
\int_{t_0}^{t} C(t)\Phi(t,\sigma) R_w(\sigma)\Phi^T(t,\sigma)C^T(\tau)d\sigma +R_v(t)\delta(t-\tau) 
\end{align*}

\section{\color{blue}{Estimation}}

After evaluating $\frac{f(x,y)}{f_y(y)}$, the result is PDF of a Guassian vector defined as:
\begin{align*}
f(x|y)=\frac{e^{-\frac{1}{2}(x-E^T[x|y])^TP^{-1}_{x|y}(x-E[x|y])}}{\sqrt{(2\pi)^ndet(P_{x|y}})}
\end{align*}
with a mean and covariance defined as follows:
\subsection{\color{ForestGreen}{Mean, $\hat{x}^+=E[x|y]$}}
\begin{align*}
E[x|y]&=\bar{x}+P_{xy}P^{-1}_y(y-\bar{y})\\
\end{align*}
\subsection{\color{ForestGreen}{Covariance, $P^+=P_{x|y}$}}
\begin{align*}
P_{x|y}&=P_x+P_{xy}P^{-1}_yP_{yx}
\end{align*}

Basic estimation procedure is: 
\begin{enumerate}
\item determine $\hat{x}^-=\bar{x}$ which is an estimate of the state
	\begin{itemize}
	\item based off of state equations 
	\item affected by state uncertainty, $w(t)$
	\end{itemize}
\item collect measurments from output equation $y(t)$
\item update estimate of state ($\hat{x}^-$) based off of new info from $y(t)$
	\begin{itemize}
	\item based off of output equation
	\item affected by sensor uncertainty, $v(t)$
	\end{itemize}
\end{enumerate}
\subsection{\color{ForestGreen}{Batch Process}}
Measurments are incorporated simultaneously.
\begin{align*}
\hat{x}^+&=\hat{x}^-+k(z-C\hat{x}^-)\\
P^+&=P^-C^T(CP^-C^T+R)^{-1}
\end{align*}
where, $k=P^-C^T(CP^-C^T+R)^{-1}$
or in an algebraically equivalent form,
\begin{align*}
\hat{x}^+&=(P^+(P^-)^{-1})\hat{x}^-+(P^+C^TR^{-1})z\\
P^+&=((P^-)^{-1}+C^TR^{-1}C)^{-1}
\end{align*}
where $R$ is the covariance of the sensor measurment from $y(t)$

also note: $z=y$ 
\subsection{\color{ForestGreen}{Sequential Process}}
Measurments are incorporated recursively. 


\end{multicols}
\end{document}